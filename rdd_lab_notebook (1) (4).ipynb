{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# SETUP: Install and Initialize PySpark"
      ],
      "metadata": {
        "id": "Fo9UUEhP1yPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspark\n",
        "# !pip install py4j\n"
      ],
      "metadata": {
        "id": "ypgTIsz71257"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n"
      ],
      "metadata": {
        "id": "FbOaPLZM15gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Spark"
      ],
      "metadata": {
        "id": "z3PvZYSk19tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName(\"RDD_Lab\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "print(f\"Spark Version: {sc.version}\")\n",
        "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmlr1idZ17jR",
        "outputId": "dfd2bc25-1842-4933-ccf4-92b555bee504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 3.5.1\n",
            "Default Parallelism: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1: CREATING RDDs - Three Main Ways"
      ],
      "metadata": {
        "id": "OgWu-c6x2AzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METHOD 1: Parallelized Collections (from Python data structures)"
      ],
      "metadata": {
        "id": "X1KOBkFh2KZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From list\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "numbers_rdd = sc.parallelize(numbers)\n",
        "print(f\"Numbers RDD: {numbers_rdd.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOMOOA082Rlb",
        "outputId": "440bbd75-cdd9-4f6e-f1b6-5bc0c3f7315c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numbers RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From list with specific number of partitions\n",
        "numbers_rdd_4_partitions = sc.parallelize(numbers, 4)\n",
        "print(f\"Partitions: {numbers_rdd_4_partitions.getNumPartitions()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGIFCiV02Tmm",
        "outputId": "3d08aec0-35ec-420c-d6e8-ae6a8bfd4fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From tuple\n",
        "data_tuples = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "people_rdd = sc.parallelize(data_tuples)\n",
        "print(f\"People RDD: {people_rdd.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLM6tbO62VMb",
        "outputId": "1ab059eb-8d69-4e99-be87-dc154e943eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "People RDD: [('Alice', 25), ('Bob', 30), ('Charlie', 35)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From range\n",
        "range_rdd = sc.parallelize(range(1, 11))\n",
        "print(f\"Range RDD: {range_rdd.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnb5tET42FS1",
        "outputId": "2624e27e-fba8-48f4-ed67-a49b6d9e2785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METHOD 2: External Datasets (from files)"
      ],
      "metadata": {
        "id": "Q0w8c19i2dez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample files for demonstration\n",
        "sample_text = \"\"\"Apache Spark is a fast engine for large-scale data processing\n",
        "It provides high-level APIs in Java, Scala, Python and R\n",
        "Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone\n",
        "It can access diverse data sources\"\"\"\n",
        "\n",
        "# Write sample file\n",
        "with open('/tmp/sample.txt', 'w') as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "# Read text file\n",
        "text_rdd = sc.textFile('/tmp/sample.txt')\n",
        "print(f\"Text file lines: {text_rdd.count()}\")\n",
        "print(\"First line:\", text_rdd.first())\n",
        "\n",
        "# Read with minimum partitions\n",
        "text_rdd_min_parts = sc.textFile('/tmp/sample.txt', minPartitions=2)\n",
        "print(f\"Partitions with minPartitions=2: {text_rdd_min_parts.getNumPartitions()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA1y6scm2cZg",
        "outputId": "12ea45bb-486d-4cfc-ffd6-530243359b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text file lines: 4\n",
            "First line: Apache Spark is a fast engine for large-scale data processing\n",
            "Partitions with minPartitions=2: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample data file (CSV-like)\n",
        "sample_data = \"\"\"name,age,city\n",
        "John,28,NYC\n",
        "Jane,32,LA\n",
        "Mike,25,Chicago\n",
        "Sarah,29,Boston\"\"\"\n",
        "\n",
        "with open('/tmp/sample_data.txt', 'w') as f:\n",
        "    f.write(sample_data)\n",
        "\n",
        "data_rdd = sc.textFile('/tmp/sample_data.txt')\n",
        "print(f\"Data file content: {data_rdd.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-tuVo422riA",
        "outputId": "3bfd25c4-eebb-4138-d17a-a0c6e8ab0924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data file content: ['name,age,city', 'John,28,NYC', 'Jane,32,LA', 'Mike,25,Chicago', 'Sarah,29,Boston']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METHOD 3: Transformations from Existing RDDs"
      ],
      "metadata": {
        "id": "feH_aMlV2u1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map transformation\n",
        "squared_rdd = numbers_rdd.map(lambda x: x**2)\n",
        "print(f\"Squared numbers: {squared_rdd.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-KeN2nV2ukT",
        "outputId": "5c2e088f-990e-423d-bff7-d8b586c92fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared numbers: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter transformation\n",
        "even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
        "print(f\"Even numbers: {even_rdd.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OevQ1BKZ2z1B",
        "outputId": "aaf4d432-617b-49fc-8bcf-3308020dfb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Even numbers: [2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FlatMap transformation\n",
        "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
        "print(f\"Total words: {words_rdd.count()}\")\n",
        "print(f\"First 10 words: {words_rdd.take(10)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WoR8e2L219O",
        "outputId": "60645529-0772-4a4b-c5c0-6de03821c1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 34\n",
            "First 10 words: ['Apache', 'Spark', 'is', 'a', 'fast', 'engine', 'for', 'large-scale', 'data', 'processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize Spark Context**"
      ],
      "metadata": {
        "id": "NoR1yEeh3Qwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create Spark configuration\n",
        "conf = SparkConf().setAppName(\"RDD Operations Tutorial\").setMaster(\"local[*]\")\n",
        "\n",
        "# Initialize Spark Context (if not already initialized)\n",
        "try:\n",
        "    sc.stop()  # Stop existing context if any\n",
        "except:\n",
        "    pass\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "print(\"✅ Spark Context initialized successfully!\")\n",
        "print(f\"Spark Version: {sc.version}\")\n",
        "print(f\"Python Version: {sc.pythonVer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLwacRm93QDQ",
        "outputId": "d666dd09-0e0a-4cdc-cac7-3339a4e3cfb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark Context initialized successfully!\n",
            "Spark Version: 3.5.1\n",
            "Python Version: 3.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Sample Data"
      ],
      "metadata": {
        "id": "dqJxHZa33YHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample datasets we'll use throughout this tutorial\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "words = sc.parallelize([\"hello\", \"world\", \"spark\", \"python\", \"big\", \"data\"])\n",
        "sentences = sc.parallelize([\"hello world\", \"spark is fast\", \"big data processing\"])\n",
        "key_value_pairs = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"c\", 1), (\"b\", 4)])\n",
        "\n",
        "print(\"Sample datasets created!\")\n",
        "print(f\"Numbers: {numbers.collect()}\")\n",
        "print(f\"Words: {words.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hosTil2m3P_2",
        "outputId": "4f6d948e-49cf-460e-813f-a48339bcef74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample datasets created!\n",
            "Numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Words: ['hello', 'world', 'spark', 'python', 'big', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is a transformation - nothing happens yet!\n",
        "doubled = numbers.map(lambda x: x * 2)\n",
        "print(\"Transformation defined, but not executed yet\")\n",
        "\n",
        "# This is an action - now everything gets executed!\n",
        "result = doubled.collect()\n",
        "print(f\"Action triggered! Result: {result}\")"
      ],
      "metadata": {
        "id": "SyKDnmtX3eI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a transformation - nothing happens yet!\n",
        "doubled = numbers.map(lambda x: x * 2)\n",
        "print(\"Transformation defined, but not executed yet\")\n",
        "\n",
        "# This is an action - now everything gets executed!\n",
        "result = doubled.collect()\n",
        "print(f\"Action triggered! Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teSHi4AI3P9Q",
        "outputId": "3ad3261b-7c13-42b9-e1e7-0c2aaae55bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation defined, but not executed yet\n",
            "Action triggered! Result: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRANSFORMATION OPERATIONS**"
      ],
      "metadata": {
        "id": "E3zMPM093spg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.map(func) - One-to-One Mapping\n",
        "\n",
        "Purpose: Transform each element individually"
      ],
      "metadata": {
        "id": "WkHL3KoZ3yjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== MAP TRANSFORMATION ===\")\n",
        "\n",
        "# Example 1: Simple arithmetic\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "doubled = numbers.map(lambda x: x * 2)\n",
        "squared = numbers.map(lambda x: x ** 2)\n",
        "\n",
        "print(f\"Original: {numbers.collect()}\")\n",
        "print(f\"Doubled:  {doubled.collect()}\")\n",
        "print(f\"Squared:  {squared.collect()}\")\n",
        "\n",
        "# Example 2: String operations\n",
        "words = sc.parallelize([\"hello\", \"world\", \"spark\"])\n",
        "upper_case = words.map(lambda word: word.upper())\n",
        "lengths = words.map(lambda word: len(word))\n",
        "\n",
        "print(f\"Original: {words.collect()}\")\n",
        "print(f\"Uppercase: {upper_case.collect()}\")\n",
        "print(f\"Lengths: {lengths.collect()}\")\n",
        "\n",
        "# Example 3: Working with dictionaries\n",
        "people = sc.parallelize([\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"age\": 35}\n",
        "])\n",
        "\n",
        "age_groups = people.map(lambda p:\n",
        "    f\"{p['name']}: Young\" if p['age'] < 30 else f\"{p['name']}: Mature\"\n",
        ")\n",
        "print(f\"Age Groups: {age_groups.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CumvgqY3P7I",
        "outputId": "128c9c80-a067-4b95-b526-97fdd0e668a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MAP TRANSFORMATION ===\n",
            "Original: [1, 2, 3, 4, 5]\n",
            "Doubled:  [2, 4, 6, 8, 10]\n",
            "Squared:  [1, 4, 9, 16, 25]\n",
            "Original: ['hello', 'world', 'spark']\n",
            "Uppercase: ['HELLO', 'WORLD', 'SPARK']\n",
            "Lengths: [5, 5, 5]\n",
            "Age Groups: ['Alice: Young', 'Bob: Mature', 'Charlie: Mature']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.flatMap(func) - One-to-Many Mapping\n",
        "\n",
        "Purpose: Transform each element into multiple elements, then flatten"
      ],
      "metadata": {
        "id": "9OK2i_CO4CjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== FLATMAP TRANSFORMATION ===\")\n",
        "\n",
        "# Example 1: Split sentences into words\n",
        "sentences = sc.parallelize([\"hello world\", \"spark is awesome\", \"big data\"])\n",
        "words = sentences.flatMap(lambda line: line.split(\" \"))\n",
        "print(f\"Sentences: {sentences.collect()}\")\n",
        "print(f\"Words: {words.collect()}\")\n",
        "\n",
        "# Example 2: Generate ranges\n",
        "nums = sc.parallelize([1, 2, 3])\n",
        "ranges = nums.flatMap(lambda n: range(1, n + 1))\n",
        "print(f\"Original: {nums.collect()}\")\n",
        "print(f\"Ranges: {ranges.collect()}\")\n",
        "\n",
        "# Example 3: Process structured data\n",
        "data = sc.parallelize([\"a:1,2,3\", \"b:4,5\", \"c:6\"])\n",
        "key_values = data.flatMap(lambda line: [\n",
        "    (line.split(\":\")[0], int(val))\n",
        "    for val in line.split(\":\")[1].split(\",\")\n",
        "])\n",
        "print(f\"Key-Values: {key_values.collect()}\")\n",
        "\n",
        "# Example 4: Text processing\n",
        "text_data = sc.parallelize([\n",
        "    \"Apache Spark is fast\",\n",
        "    \"Python is great for data science\",\n",
        "    \"Machine learning with PySpark\"\n",
        "])\n",
        "# Extract words longer than 4 characters\n",
        "long_words = text_data.flatMap(lambda line: [\n",
        "    word.lower() for word in line.split() if len(word) > 4\n",
        "])\n",
        "print(f\"Long words: {long_words.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4rDGw8c3P5e",
        "outputId": "2f628e89-13b4-41ea-e2c3-b4f44eb750dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FLATMAP TRANSFORMATION ===\n",
            "Sentences: ['hello world', 'spark is awesome', 'big data']\n",
            "Words: ['hello', 'world', 'spark', 'is', 'awesome', 'big', 'data']\n",
            "Original: [1, 2, 3]\n",
            "Ranges: [1, 1, 2, 1, 2, 3]\n",
            "Key-Values: [('a', 1), ('a', 2), ('a', 3), ('b', 4), ('b', 5), ('c', 6)]\n",
            "Long words: ['apache', 'spark', 'python', 'great', 'science', 'machine', 'learning', 'pyspark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.filter(func) - Conditional Filtering\n",
        "\n",
        "Purpose: Keep only elements that satisfy a condition"
      ],
      "metadata": {
        "id": "5NHbndh64JwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== FILTER TRANSFORMATION ===\")\n",
        "\n",
        "# Example 1: Filter numbers\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "even_numbers = numbers.filter(lambda x: x % 2 == 0)\n",
        "large_numbers = numbers.filter(lambda x: x > 5)\n",
        "\n",
        "print(f\"Original: {numbers.collect()}\")\n",
        "print(f\"Even: {even_numbers.collect()}\")\n",
        "print(f\"Greater than 5: {large_numbers.collect()}\")\n",
        "\n",
        "# Example 2: Filter strings\n",
        "words = sc.parallelize([\"spark\", \"hadoop\", \"python\", \"scala\", \"java\"])\n",
        "long_words = words.filter(lambda word: len(word) > 5)\n",
        "words_with_a = words.filter(lambda word: \"a\" in word)\n",
        "\n",
        "print(f\"Original: {words.collect()}\")\n",
        "print(f\"Long words: {long_words.collect()}\")\n",
        "print(f\"Contains 'a': {words_with_a.collect()}\")\n",
        "\n",
        "# Example 3: Complex filtering with dictionaries\n",
        "students = sc.parallelize([\n",
        "    {\"name\": \"Alice\", \"grade\": 85, \"subject\": \"Math\"},\n",
        "    {\"name\": \"Bob\", \"grade\": 92, \"subject\": \"Science\"},\n",
        "    {\"name\": \"Charlie\", \"grade\": 78, \"subject\": \"Math\"},\n",
        "    {\"name\": \"Diana\", \"grade\": 96, \"subject\": \"Science\"}\n",
        "])\n",
        "\n",
        "high_performers = students.filter(lambda s: s[\"grade\"] >= 90)\n",
        "math_students = students.filter(lambda s: s[\"subject\"] == \"Math\")\n",
        "\n",
        "print(f\"High performers: {[s['name'] for s in high_performers.collect()]}\")\n",
        "print(f\"Math students: {[s['name'] for s in math_students.collect()]}\")\n",
        "\n",
        "# Example 4: Filter with multiple conditions\n",
        "sales_data = sc.parallelize([\n",
        "    {\"product\": \"laptop\", \"price\": 1200, \"quantity\": 5},\n",
        "    {\"product\": \"phone\", \"price\": 800, \"quantity\": 10},\n",
        "    {\"product\": \"tablet\", \"price\": 600, \"quantity\": 3},\n",
        "    {\"product\": \"monitor\", \"price\": 300, \"quantity\": 8}\n",
        "])\n",
        "\n",
        "expensive_popular = sales_data.filter(\n",
        "    lambda item: item[\"price\"] > 500 and item[\"quantity\"] > 5\n",
        ")\n",
        "print(f\"Expensive & Popular: {expensive_popular.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kK1S6LK3P1p",
        "outputId": "0087c9d6-f1e7-40d4-adef-55e4c38eb013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FILTER TRANSFORMATION ===\n",
            "Original: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Even: [2, 4, 6, 8, 10]\n",
            "Greater than 5: [6, 7, 8, 9, 10]\n",
            "Original: ['spark', 'hadoop', 'python', 'scala', 'java']\n",
            "Long words: ['hadoop', 'python']\n",
            "Contains 'a': ['spark', 'hadoop', 'scala', 'java']\n",
            "High performers: ['Bob', 'Diana']\n",
            "Math students: ['Alice', 'Charlie']\n",
            "Expensive & Popular: [{'product': 'phone', 'price': 800, 'quantity': 10}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.mapPartitions(func) - Partition-Level Processing\n",
        "\n",
        "Purpose: Process entire partitions at once (efficient for expensive setup)"
      ],
      "metadata": {
        "id": "LNQQ9M9-4Px0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== MAPPARTITIONS TRANSFORMATION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8], 3)  # 3 partitions\n",
        "\n",
        "# Example 1: Expensive setup simulation\n",
        "def expensive_processing(partition):\n",
        "    print(f\"Processing partition with expensive setup...\")\n",
        "    # Simulate expensive setup (database connection, etc.)\n",
        "    expensive_resource = \"DatabaseConnection\"\n",
        "\n",
        "    # Process all elements in this partition\n",
        "    return [x * 10 for x in partition]\n",
        "\n",
        "processed = numbers.mapPartitions(expensive_processing)\n",
        "print(f\"Result: {processed.collect()}\")\n",
        "\n",
        "# Example 2: Batch processing with statistics\n",
        "def batch_statistics(partition):\n",
        "    batch = list(partition)\n",
        "    if not batch:\n",
        "        return []\n",
        "\n",
        "    batch_sum = sum(batch)\n",
        "    batch_count = len(batch)\n",
        "    batch_avg = batch_sum / batch_count\n",
        "\n",
        "    print(f\"Processing batch of size: {batch_count}, avg: {batch_avg}\")\n",
        "    return [(x, batch_avg) for x in batch]\n",
        "\n",
        "batch_processed = numbers.mapPartitions(batch_statistics)\n",
        "print(f\"Batch processed: {batch_processed.collect()}\")\n",
        "\n",
        "# Example 3: Using libraries efficiently\n",
        "def normalize_partition(partition):\n",
        "    # Convert partition to numpy array (efficient for mathematical operations)\n",
        "    import numpy as np\n",
        "    data = list(partition)\n",
        "    if not data:\n",
        "        return []\n",
        "\n",
        "    arr = np.array(data)\n",
        "    normalized = (arr - arr.mean()) / arr.std() if arr.std() > 0 else arr\n",
        "    return normalized.tolist()\n",
        "\n",
        "normalized = numbers.mapPartitions(normalize_partition)\n",
        "print(f\"Normalized: {normalized.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYg1svoi3Pyt",
        "outputId": "5cb7a463-ff93-4705-a2b2-c0dd9199ce67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MAPPARTITIONS TRANSFORMATION ===\n",
            "Result: [10, 20, 30, 40, 50, 60, 70, 80]\n",
            "Batch processed: [(1, 1.5), (2, 1.5), (3, 3.5), (4, 3.5), (5, 6.5), (6, 6.5), (7, 6.5), (8, 6.5)]\n",
            "Normalized: [-1.0, 1.0, -1.0, 1.0, -1.3416407864998738, -0.4472135954999579, 0.4472135954999579, 1.3416407864998738]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.mapPartitionsWithIndex(func) - Partition Processing with Index\n",
        "\n",
        "Purpose: Process partitions with access to partition index"
      ],
      "metadata": {
        "id": "4avFr8zY4XFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== MAPPARTITIONSWITHINDEX TRANSFORMATION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
        "\n",
        "def indexed_processing(partition_id, partition):\n",
        "    print(f\"Processing partition {partition_id}\")\n",
        "    return [(partition_id, x * 10) for x in partition]\n",
        "\n",
        "indexed_result = numbers.mapPartitionsWithIndex(indexed_processing)\n",
        "print(f\"Indexed result: {indexed_result.collect()}\")\n",
        "\n",
        "# Example: Add partition info to data\n",
        "def add_partition_info(partition_id, partition):\n",
        "    return [f\"P{partition_id}: {value}\" for value in partition]\n",
        "\n",
        "with_partition_info = numbers.mapPartitionsWithIndex(add_partition_info)\n",
        "print(f\"With partition info: {with_partition_info.collect()}\")\n",
        "\n",
        "# Example: Partition-specific processing\n",
        "def partition_specific_logic(partition_id, partition):\n",
        "    multiplier = partition_id + 1  # Different multiplier per partition\n",
        "    return [x * multiplier for x in partition]\n",
        "\n",
        "partition_specific = numbers.mapPartitionsWithIndex(partition_specific_logic)\n",
        "print(f\"Partition-specific: {partition_specific.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDV1KnfU3PwO",
        "outputId": "adfcfbb2-e3d9-4337-ea51-470d0293bc8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MAPPARTITIONSWITHINDEX TRANSFORMATION ===\n",
            "Indexed result: [(0, 10), (0, 20), (1, 30), (1, 40), (2, 50), (2, 60)]\n",
            "With partition info: ['P0: 1', 'P0: 2', 'P1: 3', 'P1: 4', 'P2: 5', 'P2: 6']\n",
            "Partition-specific: [1, 2, 6, 8, 15, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Set Operations"
      ],
      "metadata": {
        "id": "Ysq65YhR4crX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SET OPERATIONS ===\")\n",
        "\n",
        "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd2 = sc.parallelize([4, 5, 6, 7, 8])\n",
        "\n",
        "print(f\"RDD1: {rdd1.collect()}\")\n",
        "print(f\"RDD2: {rdd2.collect()}\")\n",
        "\n",
        "# Union: Combine all elements (duplicates included)\n",
        "union_result = rdd1.union(rdd2)\n",
        "print(f\"Union: {union_result.collect()}\")\n",
        "\n",
        "# Intersection: Common elements only\n",
        "intersection_result = rdd1.intersection(rdd2)\n",
        "print(f\"Intersection: {intersection_result.collect()}\")\n",
        "\n",
        "# Subtract: Elements in rdd1 but not in rdd2\n",
        "subtract_result = rdd1.subtract(rdd2)\n",
        "print(f\"Subtract (RDD1 - RDD2): {subtract_result.collect()}\")\n",
        "\n",
        "# Distinct: Remove duplicates\n",
        "with_duplicates = sc.parallelize([1, 2, 2, 3, 3, 3, 4, 5])\n",
        "distinct_result = with_duplicates.distinct()\n",
        "print(f\"With duplicates: {with_duplicates.collect()}\")\n",
        "print(f\"Distinct: {distinct_result.collect()}\")\n",
        "\n",
        "# Cartesian product\n",
        "small_rdd1 = sc.parallelize([1, 2])\n",
        "small_rdd2 = sc.parallelize([\"a\", \"b\"])\n",
        "cartesian_result = small_rdd1.cartesian(small_rdd2)\n",
        "print(f\"Cartesian product: {cartesian_result.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax9dJZrJ3Pt2",
        "outputId": "76292735-9df1-4e75-e0cc-0499ec891c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SET OPERATIONS ===\n",
            "RDD1: [1, 2, 3, 4, 5]\n",
            "RDD2: [4, 5, 6, 7, 8]\n",
            "Union: [1, 2, 3, 4, 5, 4, 5, 6, 7, 8]\n",
            "Intersection: [4, 5]\n",
            "Subtract (RDD1 - RDD2): [1, 2, 3]\n",
            "With duplicates: [1, 2, 2, 3, 3, 3, 4, 5]\n",
            "Distinct: [2, 4, 1, 3, 5]\n",
            "Cartesian product: [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Key-Value Transformations"
      ],
      "metadata": {
        "id": "s5_HuQsn4hRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== KEY-VALUE TRANSFORMATIONS ===\")\n",
        "\n",
        "pairs = sc.parallelize([\n",
        "    (\"apple\", 5), (\"banana\", 3), (\"apple\", 2),\n",
        "    (\"orange\", 4), (\"banana\", 1), (\"apple\", 3)\n",
        "])\n",
        "\n",
        "# groupByKey: Group values by key (⚠️ Expensive - causes shuffle)\n",
        "grouped = pairs.groupByKey()\n",
        "print(\"Grouped by key:\")\n",
        "for key, values in grouped.collect():\n",
        "    print(f\"  {key}: {list(values)}\")\n",
        "\n",
        "# reduceByKey: Better than groupByKey - aggregates locally first\n",
        "sum_by_key = pairs.reduceByKey(lambda a, b: a + b)\n",
        "print(f\"Sum by key: {sum_by_key.collect()}\")\n",
        "\n",
        "max_by_key = pairs.reduceByKey(lambda a, b: max(a, b))\n",
        "print(f\"Max by key: {max_by_key.collect()}\")\n",
        "\n",
        "# mapValues: Apply function to values only\n",
        "doubled_values = pairs.mapValues(lambda x: x * 2)\n",
        "print(f\"Doubled values: {doubled_values.collect()}\")\n",
        "\n",
        "# keys and values: Extract keys or values\n",
        "keys_only = pairs.keys()\n",
        "values_only = pairs.values()\n",
        "print(f\"Keys: {keys_only.collect()}\")\n",
        "print(f\"Values: {values_only.collect()}\")\n",
        "\n",
        "# sortByKey: Sort by keys\n",
        "sorted_pairs = pairs.sortByKey()\n",
        "print(f\"Sorted by key: {sorted_pairs.collect()}\")\n",
        "\n",
        "# sortBy: Sort by custom function\n",
        "sorted_by_value = pairs.sortBy(lambda x: x[1])  # Sort by value\n",
        "print(f\"Sorted by value: {sorted_by_value.collect()}\")\n",
        "\n",
        "# Join operations\n",
        "prices = sc.parallelize([(\"apple\", 1.2), (\"banana\", 0.8), (\"orange\", 1.5)])\n",
        "quantities = sc.parallelize([(\"apple\", 10), (\"banana\", 15), (\"orange\", 8)])\n",
        "\n",
        "# Inner join\n",
        "joined = prices.join(quantities)\n",
        "print(\"Price-Quantity join:\")\n",
        "for item, (price, qty) in joined.collect():\n",
        "    print(f\"  {item}: ${price} x {qty} = ${price * qty}\")\n",
        "\n",
        "# Left outer join\n",
        "left_joined = prices.leftOuterJoin(quantities)\n",
        "print(\"Left outer join:\")\n",
        "for item, (price, qty) in left_joined.collect():\n",
        "    qty_str = str(qty) if qty is not None else \"N/A\"\n",
        "    print(f\"  {item}: ${price} x {qty_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpGnUOAr3Pr1",
        "outputId": "a9416c43-8eb7-4205-e914-06962f646848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== KEY-VALUE TRANSFORMATIONS ===\n",
            "Grouped by key:\n",
            "  apple: [5, 2, 3]\n",
            "  banana: [3, 1]\n",
            "  orange: [4]\n",
            "Sum by key: [('apple', 10), ('banana', 4), ('orange', 4)]\n",
            "Max by key: [('apple', 5), ('banana', 3), ('orange', 4)]\n",
            "Doubled values: [('apple', 10), ('banana', 6), ('apple', 4), ('orange', 8), ('banana', 2), ('apple', 6)]\n",
            "Keys: ['apple', 'banana', 'apple', 'orange', 'banana', 'apple']\n",
            "Values: [5, 3, 2, 4, 1, 3]\n",
            "Sorted by key: [('apple', 5), ('apple', 2), ('apple', 3), ('banana', 3), ('banana', 1), ('orange', 4)]\n",
            "Sorted by value: [('banana', 1), ('apple', 2), ('banana', 3), ('apple', 3), ('orange', 4), ('apple', 5)]\n",
            "Price-Quantity join:\n",
            "  apple: $1.2 x 10 = $12.0\n",
            "  banana: $0.8 x 15 = $12.0\n",
            "  orange: $1.5 x 8 = $12.0\n",
            "Left outer join:\n",
            "  apple: $1.2 x 10\n",
            "  banana: $0.8 x 15\n",
            "  orange: $1.5 x 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Partitioning Operations"
      ],
      "metadata": {
        "id": "T5vo5Ccx4msa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PARTITIONING OPERATIONS ===\")\n",
        "\n",
        "data = sc.parallelize(list(range(1, 11)), 4)\n",
        "print(f\"Original partitions: {data.getNumPartitions()}\")\n",
        "print(f\"Elements per partition: {data.glom().collect()}\")\n",
        "\n",
        "# coalesce: Reduce partitions without full shuffle\n",
        "coalesced = data.coalesce(2)\n",
        "print(f\"After coalesce: {coalesced.getNumPartitions()}\")\n",
        "print(f\"Elements per partition: {coalesced.glom().collect()}\")\n",
        "\n",
        "# repartition: Change partitions with full shuffle\n",
        "repartitioned = data.repartition(6)\n",
        "print(f\"After repartition: {repartitioned.getNumPartitions()}\")\n",
        "\n",
        "# partitionBy: Custom partitioning for key-value RDDs\n",
        "key_value = sc.parallelize([(i, i*2) for i in range(10)])\n",
        "partitioned_kv = key_value.partitionBy(3, lambda k: k % 3)\n",
        "print(f\"Custom partitioned: {partitioned_kv.glom().collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo51Len33Pp-",
        "outputId": "8cfd923e-51d4-4f1a-b015-381471b78930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PARTITIONING OPERATIONS ===\n",
            "Original partitions: 4\n",
            "Elements per partition: [[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]\n",
            "After coalesce: 2\n",
            "Elements per partition: [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10]]\n",
            "After repartition: 6\n",
            "Custom partitioned: [[(0, 0), (3, 6), (6, 12), (9, 18)], [(1, 2), (4, 8), (7, 14)], [(2, 4), (5, 10), (8, 16)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Sampling Operations"
      ],
      "metadata": {
        "id": "2lqR74FR4sOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SAMPLING OPERATIONS ===\")\n",
        "\n",
        "large_numbers = sc.parallelize(range(1, 101))\n",
        "\n",
        "# sample: Random sampling\n",
        "sampled = large_numbers.sample(withReplacement=False, fraction=0.1, seed=42)\n",
        "print(f\"Sample (10%): {sorted(sampled.collect())}\")\n",
        "\n",
        "# takeSample: Take exact number of samples\n",
        "exact_sample = large_numbers.takeSample(withReplacement=False, num=5, seed=42)\n",
        "print(f\"Exact 5 samples: {sorted(exact_sample)}\")\n",
        "\n",
        "# sampleByKey: Sample by key for key-value RDDs\n",
        "kv_data = sc.parallelize([(\"A\", i) for i in range(20)] + [(\"B\", i) for i in range(20)])\n",
        "sampled_by_key = kv_data.sampleByKey(withReplacement=False, fractions={\"A\": 0.3, \"B\": 0.2})\n",
        "print(\"Sampled by key:\")\n",
        "for key, value in sampled_by_key.collect():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-9XtvmN3Pl0",
        "outputId": "8e11a1a0-c634-4cff-cb26-49c3c44bf193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SAMPLING OPERATIONS ===\n",
            "Sample (10%): [1, 4, 11, 18, 19, 33, 36, 57, 68, 71, 74, 80, 89, 95]\n",
            "Exact 5 samples: [5, 34, 35, 60, 74]\n",
            "Sampled by key:\n",
            "  A: 2\n",
            "  A: 4\n",
            "  A: 13\n",
            "  A: 16\n",
            "  A: 19\n",
            "  B: 6\n",
            "  B: 9\n",
            "  B: 14\n",
            "  B: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTION OPERATIONS**"
      ],
      "metadata": {
        "id": "9_oMfMgv4xzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.count() - Count Elements"
      ],
      "metadata": {
        "id": "F-YBZXer40_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== COUNT ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "words = sc.parallelize([\"spark\", \"python\", \"big\", \"data\"])\n",
        "\n",
        "print(f\"Numbers count: {numbers.count()}\")\n",
        "print(f\"Words count: {words.count()}\")\n",
        "print(f\"Even numbers count: {numbers.filter(lambda x: x % 2 == 0).count()}\")\n",
        "\n",
        "# countApprox: Approximate count for large datasets\n",
        "large_data = sc.parallelize(range(1000000))\n",
        "approx_count = large_data.countApprox(timeout=1000, confidence=0.95)\n",
        "print(f\"Approximate count: {approx_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YpB5X3V3Pi1",
        "outputId": "dc1e7769-15f2-471d-f170-eda2c3954c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COUNT ACTION ===\n",
            "Numbers count: 5\n",
            "Words count: 4\n",
            "Even numbers count: 2\n",
            "Approximate count: 1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.collect() - Bring All Data to Driver\n",
        "\n",
        "Use with caution for large datasets!"
      ],
      "metadata": {
        "id": "hxoby3jG45mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== COLLECT ACTION ===\")\n",
        "\n",
        "small = sc.parallelize([1, 2, 3, 4, 5])\n",
        "result = small.collect()\n",
        "print(f\"Collected: {result}\")\n",
        "print(f\"Type: {type(result)}\")\n",
        "\n",
        "# Don't do this with large datasets!\n",
        "# large = sc.parallelize(range(1000000))\n",
        "# bad_idea = large.collect()  # This could crash your driver!\n",
        "\n",
        "# Better alternatives for large datasets\n",
        "print(\"Better alternatives:\")\n",
        "print(f\"First 10 elements: {small.take(10)}\")\n",
        "print(f\"Sample of elements: {small.sample(False, 0.5).collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN3K_Vff3PgV",
        "outputId": "a794dd78-aaa7-4f95-83e3-ae5d70abbb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COLLECT ACTION ===\n",
            "Collected: [1, 2, 3, 4, 5]\n",
            "Type: <class 'list'>\n",
            "Better alternatives:\n",
            "First 10 elements: [1, 2, 3, 4, 5]\n",
            "Sample of elements: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.take(n) - Get First N Elements"
      ],
      "metadata": {
        "id": "chgjInLJ5ATq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== TAKE ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([10, 5, 8, 3, 12, 7, 1, 9])\n",
        "\n",
        "first_3 = numbers.take(3)\n",
        "first_5 = numbers.take(5)\n",
        "\n",
        "print(f\"Original (showing first 8): {numbers.take(8)}\")\n",
        "print(f\"First 3: {first_3}\")\n",
        "print(f\"First 5: {first_5}\")\n",
        "\n",
        "# takeOrdered: Take smallest n elements\n",
        "smallest_3 = numbers.takeOrdered(3)\n",
        "print(f\"Smallest 3: {smallest_3}\")\n",
        "\n",
        "# takeOrdered with custom key\n",
        "words = sc.parallelize([\"spark\", \"python\", \"apache\", \"big\", \"data\"])\n",
        "shortest_3 = words.takeOrdered(3, key=len)\n",
        "print(f\"Shortest 3 words: {shortest_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIzG41v15DIX",
        "outputId": "eae8468b-09d2-414e-9b92-810e5c49587e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TAKE ACTION ===\n",
            "Original (showing first 8): [10, 5, 8, 3, 12, 7, 1, 9]\n",
            "First 3: [10, 5, 8]\n",
            "First 5: [10, 5, 8, 3, 12]\n",
            "Smallest 3: [1, 3, 5]\n",
            "Shortest 3 words: ['big', 'data', 'spark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.top(n) - Get Top N Elements"
      ],
      "metadata": {
        "id": "GRdWeedb5Hh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== TOP ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([10, 5, 8, 3, 12, 7, 1, 9])\n",
        "top_3 = numbers.top(3)\n",
        "top_5 = numbers.top(5)\n",
        "\n",
        "print(f\"Top 3: {top_3}\")\n",
        "print(f\"Top 5: {top_5}\")\n",
        "\n",
        "# Custom ordering for complex data\n",
        "people = sc.parallelize([\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"age\": 35},\n",
        "    {\"name\": \"Diana\", \"age\": 20}\n",
        "])\n",
        "\n",
        "oldest_2 = people.top(2, key=lambda person: person[\"age\"])\n",
        "print(f\"Oldest 2: {[f'{p['name']}({p['age']})' for p in oldest_2]}\")\n",
        "\n",
        "# Longest words\n",
        "words = sc.parallelize([\"spark\", \"python\", \"apache\", \"big\", \"data\", \"processing\"])\n",
        "longest_3 = words.top(3, key=len)\n",
        "print(f\"Longest 3 words: {longest_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUiPy4lq5DEA",
        "outputId": "61d6b142-7db6-4ccf-940d-563e64e94bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TOP ACTION ===\n",
            "Top 3: [12, 10, 9]\n",
            "Top 5: [12, 10, 9, 8, 7]\n",
            "Oldest 2: ['Charlie(35)', 'Bob(30)']\n",
            "Longest 3 words: ['processing', 'python', 'apache']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.countByValue() - Frequency Count"
      ],
      "metadata": {
        "id": "kGDNiSod5Lh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== COUNTBYVALUE ACTION ===\")\n",
        "\n",
        "letters = sc.parallelize([\"a\", \"b\", \"a\", \"c\", \"b\", \"a\", \"d\"])\n",
        "frequencies = letters.countByValue()\n",
        "\n",
        "print(\"Letter frequencies:\")\n",
        "for letter, count in sorted(frequencies.items()):\n",
        "    print(f\"  '{letter}': {count} times\")\n",
        "\n",
        "# With numbers\n",
        "numbers = sc.parallelize([1, 2, 1, 3, 2, 1, 4, 2])\n",
        "num_freq = numbers.countByValue()\n",
        "print(\"Number frequencies:\")\n",
        "for num in sorted(num_freq.keys()):\n",
        "    print(f\"  {num}: {num_freq[num]} times\")\n",
        "\n",
        "# With complex data\n",
        "grades = sc.parallelize([\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"B\", \"C\", \"A\"])\n",
        "grade_counts = grades.countByValue()\n",
        "print(\"Grade distribution:\")\n",
        "for grade in sorted(grade_counts.keys()):\n",
        "    print(f\"  {grade}: {grade_counts[grade]} students\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9PNQrzS5DAf",
        "outputId": "01a61dcf-7007-42d3-97ce-ac2ada077251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COUNTBYVALUE ACTION ===\n",
            "Letter frequencies:\n",
            "  'a': 3 times\n",
            "  'b': 2 times\n",
            "  'c': 1 times\n",
            "  'd': 1 times\n",
            "Number frequencies:\n",
            "  1: 3 times\n",
            "  2: 3 times\n",
            "  3: 1 times\n",
            "  4: 1 times\n",
            "Grade distribution:\n",
            "  A: 4 students\n",
            "  B: 3 students\n",
            "  C: 2 students\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.reduce(func) - Combine Elements"
      ],
      "metadata": {
        "id": "_6ar8dBO5Pym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== REDUCE ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Sum all elements\n",
        "total_sum = numbers.reduce(lambda a, b: a + b)\n",
        "print(f\"Sum: {total_sum}\")\n",
        "\n",
        "# Find maximum\n",
        "maximum = numbers.reduce(lambda a, b: max(a, b))\n",
        "print(f\"Maximum: {maximum}\")\n",
        "\n",
        "# Find minimum\n",
        "minimum = numbers.reduce(lambda a, b: min(a, b))\n",
        "print(f\"Minimum: {minimum}\")\n",
        "\n",
        "# String concatenation\n",
        "words = sc.parallelize([\"Hello\", \"World\", \"Spark\", \"Python\"])\n",
        "concatenated = words.reduce(lambda a, b: a + \" \" + b)\n",
        "print(f\"Concatenated: {concatenated}\")\n",
        "\n",
        "# Product of all numbers\n",
        "product = numbers.reduce(lambda a, b: a * b)\n",
        "print(f\"Product: {product}\")\n",
        "\n",
        "# Find longest string\n",
        "long_words = sc.parallelize([\"spark\", \"python\", \"apache\", \"data\", \"processing\"])\n",
        "longest = long_words.reduce(lambda a, b: a if len(a) > len(b) else b)\n",
        "print(f\"Longest word: {longest}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMWmy6kr5C9z",
        "outputId": "3a1fad09-1690-40bc-c9df-61a9aae86533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== REDUCE ACTION ===\n",
            "Sum: 15\n",
            "Maximum: 5\n",
            "Minimum: 1\n",
            "Concatenated: Hello World Spark Python\n",
            "Product: 120\n",
            "Longest word: processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.fold(zeroValue, func) - Reduce with Initial Value"
      ],
      "metadata": {
        "id": "WDRkn1ki5AN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== FOLD ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Sum with initial value\n",
        "sum_with_init = numbers.fold(10, lambda a, b: a + b)\n",
        "print(f\"Sum with initial value 10: {sum_with_init}\")\n",
        "\n",
        "# Product with initial value\n",
        "product = numbers.fold(1, lambda a, b: a * b)\n",
        "print(f\"Product: {product}\")\n",
        "\n",
        "# String folding\n",
        "words = sc.parallelize([\"Spark\", \"is\", \"fast\"])\n",
        "sentence = words.fold(\"Apache\", lambda a, b: a + \" \" + b)\n",
        "print(f\"Sentence: {sentence}\")\n",
        "\n",
        "# Finding maximum with initial value\n",
        "max_with_init = numbers.fold(0, lambda a, b: max(a, b))\n",
        "print(f\"Max with initial 0: {max_with_init}\")\n",
        "\n",
        "# Building a set ,Use aggregate() instead\n",
        "unique_letters = sc.parallelize([\"a\", \"b\", \"c\", \"a\", \"b\"])\n",
        "letter_set = unique_letters.aggregate(\n",
        "    set(),                                    # Initial accumulator\n",
        "    lambda acc, x: acc.union({x}),           # Combine element with accumulator\n",
        "    lambda acc1, acc2: acc1.union(acc2)      # Combine two accumulators\n",
        ")\n",
        "print(f\"Unique letters: {letter_set}\")\n",
        "\n",
        "#: Use distinct()\n",
        "unique_letters = sc.parallelize([\"a\", \"b\", \"c\", \"a\", \"b\"])\n",
        "result = set(unique_letters.distinct().collect())\n",
        "print(f\"Unique letters: {result}\")\n",
        "\n",
        "# Fix fold() by making types consistent\n",
        "unique_letters = sc.parallelize([\"a\", \"b\", \"c\", \"a\", \"b\"])\n",
        "letter_sets = unique_letters.map(lambda x: {x})  # Convert strings to sets first\n",
        "combined_set = letter_sets.fold(set(), lambda acc, x: acc.union(x))\n",
        "print(f\"Unique letters: {combined_set}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uva6wRDo5WIg",
        "outputId": "f0257625-a5d4-46e1-9c70-2d9bad16e493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FOLD ACTION ===\n",
            "Sum with initial value 10: 45\n",
            "Product: 120\n",
            "Sentence: Apache Apache Spark Apache is fast\n",
            "Max with initial 0: 5\n",
            "Unique letters: {'a', 'b', 'c'}\n",
            "Unique letters: {'b', 'a', 'c'}\n",
            "Unique letters: {'a', 'b', 'c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.aggregate() - Flexible Aggregation"
      ],
      "metadata": {
        "id": "CuDrbYMs5ALU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== AGGREGATE ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# Calculate sum and count in one pass\n",
        "def seq_op(acc, value):\n",
        "    # acc is (sum, count)\n",
        "    return (acc[0] + value, acc[1] + 1)\n",
        "\n",
        "def comb_op(acc1, acc2):\n",
        "    # Combine two accumulators\n",
        "    return (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "\n",
        "sum_count = numbers.aggregate((0, 0), seq_op, comb_op)\n",
        "total_sum, count = sum_count\n",
        "average = total_sum / count if count > 0 else 0\n",
        "\n",
        "print(f\"Sum: {total_sum}, Count: {count}, Average: {average}\")\n",
        "\n",
        "# More complex aggregation - statistics\n",
        "def stats_seq_op(stats, value):\n",
        "    # stats is [sum, count, max, min]\n",
        "    return [\n",
        "        stats[0] + value,          # sum\n",
        "        stats[1] + 1,              # count\n",
        "        max(stats[2], value),      # max\n",
        "        min(stats[3], value)       # min\n",
        "    ]\n",
        "\n",
        "def stats_comb_op(stats1, stats2):\n",
        "    return [\n",
        "        stats1[0] + stats2[0],            # sum\n",
        "        stats1[1] + stats2[1],            # count\n",
        "        max(stats1[2], stats2[2]),        # max\n",
        "        min(stats1[3], stats2[3])         # min\n",
        "    ]\n",
        "\n",
        "import sys\n",
        "initial_stats = [0, 0, -sys.maxsize, sys.maxsize]\n",
        "final_stats = numbers.aggregate(initial_stats, stats_seq_op, stats_comb_op)\n",
        "\n",
        "print(f\"Advanced Stats:\")\n",
        "print(f\"  Sum: {final_stats[0]}\")\n",
        "print(f\"  Count: {final_stats[1]}\")\n",
        "print(f\"  Max: {final_stats[2]}\")\n",
        "print(f\"  Min: {final_stats[3]}\")\n",
        "print(f\"  Average: {final_stats[0] / final_stats[1]}\")\n",
        "\n",
        "# Text analysis example\n",
        "text_data = sc.parallelize([\n",
        "    \"hello world\", \"spark is great\", \"python rocks\",\n",
        "    \"big data analytics\", \"machine learning\"\n",
        "])\n",
        "\n",
        "def text_seq_op(acc, sentence):\n",
        "    words = sentence.split()\n",
        "    return {\n",
        "        'total_chars': acc['total_chars'] + len(sentence),\n",
        "        'total_words': acc['total_words'] + len(words),\n",
        "        'sentence_count': acc['sentence_count'] + 1,\n",
        "        'longest_word': max(acc['longest_word'], max(words, key=len), key=len)\n",
        "    }\n",
        "\n",
        "def text_comb_op(acc1, acc2):\n",
        "    return {\n",
        "        'total_chars': acc1['total_chars'] + acc2['total_chars'],\n",
        "        'total_words': acc1['total_words'] + acc2['total_words'],\n",
        "        'sentence_count': acc1['sentence_count'] + acc2['sentence_count'],\n",
        "        'longest_word': max(acc1['longest_word'], acc2['longest_word'], key=len)\n",
        "    }\n",
        "\n",
        "text_stats = text_data.aggregate(\n",
        "    {'total_chars': 0, 'total_words': 0, 'sentence_count': 0, 'longest_word': ''},\n",
        "    text_seq_op,\n",
        "    text_comb_op\n",
        ")\n",
        "\n",
        "print(\"Text Statistics:\")\n",
        "print(f\"  Total characters: {text_stats['total_chars']}\")\n",
        "print(f\"  Total words: {text_stats['total_words']}\")\n",
        "print(f\"  Sentences: {text_stats['sentence_count']}\")\n",
        "print(f\"  Average words per sentence: {text_stats['total_words'] / text_stats['sentence_count']:.1f}\")\n",
        "print(f\"  Longest word: '{text_stats['longest_word']}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC0qDyMk5bZp",
        "outputId": "6cf3f69e-f091-4868-eeb0-1bb04f4197ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AGGREGATE ACTION ===\n",
            "Sum: 55, Count: 10, Average: 5.5\n",
            "Advanced Stats:\n",
            "  Sum: 55\n",
            "  Count: 10\n",
            "  Max: 10\n",
            "  Min: 1\n",
            "  Average: 5.5\n",
            "Text Statistics:\n",
            "  Total characters: 71\n",
            "  Total words: 12\n",
            "  Sentences: 5\n",
            "  Average words per sentence: 2.4\n",
            "  Longest word: 'analytics'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.foreach(func) - Side Effects"
      ],
      "metadata": {
        "id": "2dSgqJcM5fDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== FOREACH ACTION ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# Simple printing (Note: output might not appear in order due to distributed nature)\n",
        "print(\"Printing each element:\")\n",
        "numbers.foreach(lambda x: print(f\"  Value: {x}\"))\n",
        "\n",
        "# Collecting data in a shared variable (use with caution)\n",
        "collected_data = []\n",
        "\n",
        "def collect_data(value):\n",
        "    # In real distributed environment, this won't work as expected\n",
        "    # Each executor has its own copy of the list\n",
        "    collected_data.append(value * 2)\n",
        "\n",
        "numbers.foreach(collect_data)\n",
        "print(f\"Collected data (local only): {collected_data}\")\n",
        "\n",
        "# Better approach: use foreachPartition for batch operations\n",
        "def process_partition(partition):\n",
        "    batch = list(partition)\n",
        "    print(f\"Processing partition with {len(batch)} elements: {batch}\")\n",
        "    # Simulate batch processing (e.g., database insert)\n",
        "    for value in batch:\n",
        "        # Simulate some processing\n",
        "        pass\n",
        "\n",
        "numbers.foreachPartition(process_partition)\n",
        "\n",
        "# Practical example: simulating writes to external system\n",
        "def write_to_external_system(partition):\n",
        "    batch = list(partition)\n",
        "    if batch:\n",
        "        print(f\"Writing batch of {len(batch)} elements to external system\")\n",
        "        # Simulate writing to database, file, etc.\n",
        "        for value in batch:\n",
        "            print(f\"  Writing {value}\")\n",
        "\n",
        "numbers.foreachPartition(write_to_external_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz-YBtp-5bUh",
        "outputId": "fb7a6be7-7a81-4ad2-f192-3ce96bbdca91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FOREACH ACTION ===\n",
            "Printing each element:\n",
            "Collected data (local only): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Additional Useful Actions"
      ],
      "metadata": {
        "id": "4MaS2g1z6ckK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== ADDITIONAL ACTIONS ===\")\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "key_value = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"c\", 1)])\n",
        "\n",
        "# first: Get first element\n",
        "print(f\"First element: {numbers.first()}\")\n",
        "\n",
        "# isEmpty: Check if RDD is empty\n",
        "print(f\"Is empty: {numbers.isEmpty()}\")\n",
        "empty_rdd = sc.parallelize([])\n",
        "print(f\"Empty RDD is empty: {empty_rdd.isEmpty()}\")\n",
        "\n",
        "# sample: Random sampling (this returns an RDD, different from takeSample)\n",
        "sampled_rdd = numbers.sample(withReplacement=False, fraction=0.3, seed=42)\n",
        "print(f\"Sampled elements: {sampled_rdd.collect()}\")\n",
        "\n",
        "# countByKey: Count elements per key\n",
        "key_value_counts = key_value.countByKey()\n",
        "print(\"Counts by key:\")\n",
        "for key, count in sorted(key_value_counts.items()):\n",
        "    print(f\"  Key '{key}': {count} times\")\n",
        "\n",
        "# glom: Convert each partition to array\n",
        "partitioned = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
        "partition_arrays = partitioned.glom()\n",
        "print(\"Partition contents:\")\n",
        "for i, partition in enumerate(partition_arrays.collect()):\n",
        "    print(f\"  Partition {i}: {list(partition)}\")\n",
        "\n",
        "# lookup: Get values for specific keys (only for key-value RDDs)\n",
        "kv_rdd = sc.parallelize([(\"apple\", 5), (\"banana\", 3), (\"apple\", 2), (\"cherry\", 8)])\n",
        "apple_values = kv_rdd.lookup(\"apple\")\n",
        "print(f\"Values for 'apple': {apple_values}\")\n",
        "\n",
        "# min and max\n",
        "print(f\"Minimum: {numbers.min()}\")\n",
        "print(f\"Maximum: {numbers.max()}\")\n",
        "\n",
        "# variance and stdev (statistical functions)\n",
        "print(f\"Variance: {numbers.variance()}\")\n",
        "print(f\"Standard deviation: {numbers.stdev()}\")\n",
        "print(f\"Mean: {numbers.mean()}\")\n",
        "\n",
        "# histogram: Get histogram of values\n",
        "hist_data = sc.parallelize([1, 1, 2, 3, 3, 3, 4, 5, 5, 6, 7, 8, 9, 10])\n",
        "histogram = hist_data.histogram(5)  # 5 buckets\n",
        "bucket_boundaries, counts = histogram\n",
        "print(\"Histogram:\")\n",
        "for i in range(len(counts)):\n",
        "    print(f\"  [{bucket_boundaries[i]:.1f}, {bucket_boundaries[i+1]:.1f}): {counts[i]}\")\n",
        "\n",
        "# stats: Get comprehensive statistics\n",
        "stats = numbers.stats()\n",
        "print(f\"Stats: count={stats.count()}, mean={stats.mean():.2f}, stdev={stats.stdev():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CysfZrrx5bRj",
        "outputId": "6aaa1892-8cac-45aa-8716-0b5cb0b553f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ADDITIONAL ACTIONS ===\n",
            "First element: 1\n",
            "Is empty: False\n",
            "Empty RDD is empty: True\n",
            "Sampled elements: [1, 2, 4, 5]\n",
            "Counts by key:\n",
            "  Key 'a': 2 times\n",
            "  Key 'b': 1 times\n",
            "  Key 'c': 1 times\n",
            "Partition contents:\n",
            "  Partition 0: [1, 2]\n",
            "  Partition 1: [3, 4]\n",
            "  Partition 2: [5, 6]\n",
            "Values for 'apple': [5, 2]\n",
            "Minimum: 1\n",
            "Maximum: 10\n",
            "Variance: 8.25\n",
            "Standard deviation: 2.8722813232690143\n",
            "Mean: 5.5\n",
            "Histogram:\n",
            "  [1.0, 2.8): 3\n",
            "  [2.8, 4.6): 4\n",
            "  [4.6, 6.4): 3\n",
            "  [6.4, 8.2): 2\n",
            "  [8.2, 10.0): 2\n",
            "Stats: count=10, mean=5.50, stdev=2.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Count**"
      ],
      "metadata": {
        "id": "aPlEGxwy6qMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PATTERN: WORD COUNT ===\")\n",
        "\n",
        "# Create sample text data\n",
        "text_lines = sc.parallelize([\n",
        "    \"Apache Spark is fast\",\n",
        "    \"Spark can process big data\",\n",
        "    \"Big data processing with Spark\",\n",
        "    \"Python and Spark work great together\",\n",
        "    \"PySpark makes big data easy\"\n",
        "])\n",
        "\n",
        "# Classic word count implementation\n",
        "word_counts = (text_lines\n",
        "    .flatMap(lambda line: line.lower().split())  # Split into words\n",
        "    .filter(lambda word: word.isalpha())         # Remove non-alphabetic\n",
        "    .map(lambda word: (word, 1))                 # Create (word, 1) pairs\n",
        "    .reduceByKey(lambda a, b: a + b)             # Count occurrences\n",
        "    .sortBy(lambda x: x[1], ascending=False))    # Sort by count descending\n",
        "\n",
        "print(\"Word counts:\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"  '{word}': {count}\")\n",
        "\n",
        "# More advanced word count with filtering\n",
        "advanced_word_counts = (text_lines\n",
        "    .flatMap(lambda line: line.lower().split())\n",
        "    .filter(lambda word: len(word) > 3)          # Only words longer than 3 chars\n",
        "    .filter(lambda word: word.isalpha())         # Only alphabetic words\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .filter(lambda x: x[1] >= 2)                 # Only words appearing 2+ times\n",
        "    .sortBy(lambda x: x[1], ascending=False))\n",
        "\n",
        "print(\"\\nAdvanced word counts (len>3, freq>=2):\")\n",
        "for word, count in advanced_word_counts.collect():\n",
        "    print(f\"  '{word}': {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4N4c7N65bO7",
        "outputId": "d2243a8d-0b14-4090-c385-60da8ce62fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PATTERN: WORD COUNT ===\n",
            "Word counts:\n",
            "  'spark': 4\n",
            "  'big': 3\n",
            "  'data': 3\n",
            "  'apache': 1\n",
            "  'fast': 1\n",
            "  'with': 1\n",
            "  'python': 1\n",
            "  'and': 1\n",
            "  'work': 1\n",
            "  'easy': 1\n",
            "  'is': 1\n",
            "  'can': 1\n",
            "  'process': 1\n",
            "  'processing': 1\n",
            "  'great': 1\n",
            "  'together': 1\n",
            "  'pyspark': 1\n",
            "  'makes': 1\n",
            "\n",
            "Advanced word counts (len>3, freq>=2):\n",
            "  'spark': 4\n",
            "  'data': 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing Pipeline**"
      ],
      "metadata": {
        "id": "nZC7i4w86uyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PATTERN: DATA PROCESSING PIPELINE ===\")\n",
        "\n",
        "# Sample sales data\n",
        "sales_data = sc.parallelize([\n",
        "    {\"product\": \"laptop\", \"amount\": 1200.0, \"region\": \"north\", \"salesperson\": \"Alice\"},\n",
        "    {\"product\": \"phone\", \"amount\": 800.0, \"region\": \"south\", \"salesperson\": \"Bob\"},\n",
        "    {\"product\": \"laptop\", \"amount\": 1300.0, \"region\": \"north\", \"salesperson\": \"Charlie\"},\n",
        "    {\"product\": \"tablet\", \"amount\": 600.0, \"region\": \"east\", \"salesperson\": \"Diana\"},\n",
        "    {\"product\": \"phone\", \"amount\": 750.0, \"region\": \"south\", \"salesperson\": \"Alice\"},\n",
        "    {\"product\": \"laptop\", \"amount\": 1100.0, \"region\": \"west\", \"salesperson\": \"Bob\"},\n",
        "    {\"product\": \"monitor\", \"amount\": 400.0, \"region\": \"north\", \"salesperson\": \"Charlie\"}\n",
        "])\n",
        "\n",
        "# Processing pipeline\n",
        "result = (sales_data\n",
        "    .filter(lambda sale: sale[\"amount\"] > 700)                    # High-value sales only\n",
        "    .map(lambda sale: (sale[\"region\"], sale[\"amount\"]))           # Extract region and amount\n",
        "    .reduceByKey(lambda a, b: a + b)                             # Sum by region\n",
        "    .sortBy(lambda x: x[1], ascending=False))                     # Sort by total descending\n",
        "\n",
        "print(\"High-value sales by region (>$700):\")\n",
        "for region, total in result.collect():\n",
        "    print(f\"  {region}: ${total:,.2f}\")\n",
        "\n",
        "# More complex pipeline: product analysis\n",
        "product_analysis = (sales_data\n",
        "    .map(lambda sale: (sale[\"product\"], (sale[\"amount\"], 1)))     # (product, (amount, count))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))        # Sum amounts and counts\n",
        "    .mapValues(lambda x: {\n",
        "        \"total_sales\": x[0],\n",
        "        \"count\": x[1],\n",
        "        \"average\": x[0] / x[1]\n",
        "    })\n",
        "    .sortBy(lambda x: x[1][\"total_sales\"], ascending=False))\n",
        "\n",
        "print(\"\\nProduct analysis:\")\n",
        "for product, stats in product_analysis.collect():\n",
        "    print(f\"  {product}:\")\n",
        "    print(f\"    Total sales: ${stats['total_sales']:,.2f}\")\n",
        "    print(f\"    Units sold: {stats['count']}\")\n",
        "    print(f\"    Average price: ${stats['average']:,.2f}\")\n",
        "\n",
        "# Salesperson performance\n",
        "salesperson_performance = (sales_data\n",
        "    .map(lambda sale: (sale[\"salesperson\"], sale[\"amount\"]))\n",
        "    .groupByKey()\n",
        "    .mapValues(lambda amounts: {\n",
        "        \"sales\": list(amounts),\n",
        "        \"total\": sum(amounts),\n",
        "        \"count\": len(list(amounts)),\n",
        "        \"average\": sum(amounts) / len(list(amounts))\n",
        "    })\n",
        "    .sortBy(lambda x: x[1][\"total\"], ascending=False))\n",
        "\n",
        "print(\"\\nSalesperson performance:\")\n",
        "for person, stats in salesperson_performance.collect():\n",
        "    print(f\"  {person}:\")\n",
        "    print(f\"    Total: ${stats['total']:,.2f}\")\n",
        "    print(f\"    Transactions: {stats['count']}\")\n",
        "    print(f\"    Average: ${stats['average']:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmA67wR35bMK",
        "outputId": "6160d63c-9b96-4d01-8c6c-8a3700e47bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PATTERN: DATA PROCESSING PIPELINE ===\n",
            "High-value sales by region (>$700):\n",
            "  north: $2,500.00\n",
            "  south: $1,550.00\n",
            "  west: $1,100.00\n",
            "\n",
            "Product analysis:\n",
            "  laptop:\n",
            "    Total sales: $3,600.00\n",
            "    Units sold: 3\n",
            "    Average price: $1,200.00\n",
            "  phone:\n",
            "    Total sales: $1,550.00\n",
            "    Units sold: 2\n",
            "    Average price: $775.00\n",
            "  tablet:\n",
            "    Total sales: $600.00\n",
            "    Units sold: 1\n",
            "    Average price: $600.00\n",
            "  monitor:\n",
            "    Total sales: $400.00\n",
            "    Units sold: 1\n",
            "    Average price: $400.00\n",
            "\n",
            "Salesperson performance:\n",
            "  Alice:\n",
            "    Total: $1,950.00\n",
            "    Transactions: 2\n",
            "    Average: $975.00\n",
            "  Bob:\n",
            "    Total: $1,900.00\n",
            "    Transactions: 2\n",
            "    Average: $950.00\n",
            "  Charlie:\n",
            "    Total: $1,700.00\n",
            "    Transactions: 2\n",
            "    Average: $850.00\n",
            "  Diana:\n",
            "    Total: $600.00\n",
            "    Transactions: 1\n",
            "    Average: $600.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Join Operations Pattern**"
      ],
      "metadata": {
        "id": "tAZB5MEz61uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PATTERN: JOIN OPERATIONS ===\")\n",
        "\n",
        "# Sample data\n",
        "customers = sc.parallelize([\n",
        "    (1, {\"name\": \"Alice\", \"city\": \"New York\"}),\n",
        "    (2, {\"name\": \"Bob\", \"city\": \"San Francisco\"}),\n",
        "    (3, {\"name\": \"Charlie\", \"city\": \"Chicago\"}),\n",
        "    (4, {\"name\": \"Diana\", \"city\": \"Boston\"})\n",
        "])\n",
        "\n",
        "orders = sc.parallelize([\n",
        "    (1, {\"product\": \"laptop\", \"amount\": 1200}),\n",
        "    (2, {\"product\": \"phone\", \"amount\": 800}),\n",
        "    (1, {\"product\": \"tablet\", \"amount\": 600}),\n",
        "    (3, {\"product\": \"laptop\", \"amount\": 1300}),\n",
        "    (5, {\"product\": \"monitor\", \"amount\": 400})  # Customer 5 doesn't exist\n",
        "])\n",
        "\n",
        "products = sc.parallelize([\n",
        "    (\"laptop\", {\"category\": \"electronics\", \"weight\": 2.5}),\n",
        "    (\"phone\", {\"category\": \"electronics\", \"weight\": 0.2}),\n",
        "    (\"tablet\", {\"category\": \"electronics\", \"weight\": 0.5}),\n",
        "    (\"monitor\", {\"category\": \"electronics\", \"weight\": 3.0})\n",
        "])\n",
        "\n",
        "# Inner join: customers with orders\n",
        "customer_orders = customers.join(orders)\n",
        "print(\"Customer orders (inner join):\")\n",
        "for customer_id, (customer_info, order_info) in customer_orders.collect():\n",
        "    print(f\"  {customer_info['name']} from {customer_info['city']} ordered {order_info['product']}\")\n",
        "\n",
        "# Left outer join: all customers (with optional orders)\n",
        "all_customers = customers.leftOuterJoin(orders)\n",
        "print(\"\\nAll customers with their orders (left outer join):\")\n",
        "for customer_id, (customer_info, order_info) in all_customers.collect():\n",
        "    if order_info:\n",
        "        print(f\"  {customer_info['name']}: {order_info['product']}\")\n",
        "    else:\n",
        "        print(f\"  {customer_info['name']}: No orders\")\n",
        "\n",
        "# Complex join: customer orders with product details\n",
        "# First, create order-product pairs\n",
        "order_products = orders.map(lambda x: (x[1][\"product\"], (x[0], x[1])))  # (product, (customer_id, order))\n",
        "\n",
        "# Join with product information\n",
        "detailed_orders = order_products.join(products)\n",
        "print(\"\\nDetailed order information:\")\n",
        "for product, ((customer_id, order), product_info) in detailed_orders.collect():\n",
        "    print(f\"  Product: {product}, Customer ID: {customer_id}\")\n",
        "    print(f\"    Amount: ${order['amount']}, Category: {product_info['category']}\")\n",
        "\n",
        "# Three-way join simulation\n",
        "# Get customer name for each order with product details\n",
        "customer_name_map = customers.collectAsMap()  # Small dataset, safe to collect\n",
        "\n",
        "detailed_with_names = detailed_orders.map(\n",
        "    lambda x: {\n",
        "        \"customer\": customer_name_map.get(x[1][0][0], \"Unknown\"),\n",
        "        \"product\": x[0],\n",
        "        \"amount\": x[1][0][1][\"amount\"],\n",
        "        \"category\": x[1][1][\"category\"],\n",
        "        \"weight\": x[1][1][\"weight\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\nComplete order details:\")\n",
        "for order in detailed_with_names.collect():\n",
        "    print(f\"  {order['customer']} bought {order['product']} (${order['amount']}) - {order['weight']}kg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYqHHP8B1DZ0",
        "outputId": "dd2730a3-493e-41f6-c7dc-ca751acba2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PATTERN: JOIN OPERATIONS ===\n",
            "Customer orders (inner join):\n",
            "  Alice from New York ordered laptop\n",
            "  Alice from New York ordered tablet\n",
            "  Bob from San Francisco ordered phone\n",
            "  Charlie from Chicago ordered laptop\n",
            "\n",
            "All customers with their orders (left outer join):\n",
            "  Diana: No orders\n",
            "  Alice: laptop\n",
            "  Alice: tablet\n",
            "  Bob: phone\n",
            "  Charlie: laptop\n",
            "\n",
            "Detailed order information:\n",
            "  Product: laptop, Customer ID: 1\n",
            "    Amount: $1200, Category: electronics\n",
            "  Product: laptop, Customer ID: 3\n",
            "    Amount: $1300, Category: electronics\n",
            "  Product: phone, Customer ID: 2\n",
            "    Amount: $800, Category: electronics\n",
            "  Product: tablet, Customer ID: 1\n",
            "    Amount: $600, Category: electronics\n",
            "  Product: monitor, Customer ID: 5\n",
            "    Amount: $400, Category: electronics\n",
            "\n",
            "Complete order details:\n",
            "  {'name': 'Alice', 'city': 'New York'} bought laptop ($1200) - 2.5kg\n",
            "  {'name': 'Charlie', 'city': 'Chicago'} bought laptop ($1300) - 2.5kg\n",
            "  {'name': 'Bob', 'city': 'San Francisco'} bought phone ($800) - 0.2kg\n",
            "  {'name': 'Alice', 'city': 'New York'} bought tablet ($600) - 0.5kg\n",
            "  Unknown bought monitor ($400) - 3.0kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Aggregation Pattern**"
      ],
      "metadata": {
        "id": "NbksSnra67Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PATTERN: ADVANCED AGGREGATION ===\")\n",
        "\n",
        "# Sample transaction data\n",
        "transactions = sc.parallelize([\n",
        "    {\"user_id\": 1, \"amount\": 100.0, \"category\": \"food\", \"timestamp\": \"2023-01-01\"},\n",
        "    {\"user_id\": 1, \"amount\": 50.0, \"category\": \"transport\", \"timestamp\": \"2023-01-02\"},\n",
        "    {\"user_id\": 2, \"amount\": 200.0, \"category\": \"food\", \"timestamp\": \"2023-01-01\"},\n",
        "    {\"user_id\": 1, \"amount\": 30.0, \"category\": \"food\", \"timestamp\": \"2023-01-03\"},\n",
        "    {\"user_id\": 3, \"amount\": 150.0, \"category\": \"entertainment\", \"timestamp\": \"2023-01-01\"},\n",
        "    {\"user_id\": 2, \"amount\": 80.0, \"category\": \"transport\", \"timestamp\": \"2023-01-02\"},\n",
        "    {\"user_id\": 3, \"amount\": 25.0, \"category\": \"food\", \"timestamp\": \"2023-01-03\"},\n",
        "    {\"user_id\": 1, \"amount\": 300.0, \"category\": \"shopping\", \"timestamp\": \"2023-01-04\"}\n",
        "])\n",
        "\n",
        "# User spending summary using aggregateByKey\n",
        "def create_summary(amount, category):\n",
        "    return {\n",
        "        \"total\": amount,\n",
        "        \"count\": 1,\n",
        "        \"categories\": {category},\n",
        "        \"transactions\": [amount]\n",
        "    }\n",
        "\n",
        "def merge_summaries(summary1, summary2):\n",
        "    return {\n",
        "        \"total\": summary1[\"total\"] + summary2[\"total\"],\n",
        "        \"count\": summary1[\"count\"] + summary2[\"count\"],\n",
        "        \"categories\": summary1[\"categories\"].union(summary2[\"categories\"]),\n",
        "        \"transactions\": summary1[\"transactions\"] + summary2[\"transactions\"]\n",
        "    }\n",
        "\n",
        "def combine_values(summary, transaction):\n",
        "    return {\n",
        "        \"total\": summary[\"total\"] + transaction[\"amount\"],\n",
        "        \"count\": summary[\"count\"] + 1,\n",
        "        \"categories\": summary[\"categories\"].union({transaction[\"category\"]}),\n",
        "        \"transactions\": summary[\"transactions\"] + [transaction[\"amount\"]]\n",
        "    }\n",
        "\n",
        "# Create user summaries\n",
        "user_summaries = (transactions\n",
        "    .map(lambda t: (t[\"user_id\"], t))\n",
        "    .aggregateByKey(\n",
        "        {\"total\": 0, \"count\": 0, \"categories\": set(), \"transactions\": []},\n",
        "        lambda summary, transaction: combine_values(summary, transaction),\n",
        "        lambda s1, s2: merge_summaries(s1, s2)\n",
        "    ))\n",
        "\n",
        "print(\"User spending summaries:\")\n",
        "for user_id, summary in sorted(user_summaries.collect()):\n",
        "    avg_transaction = summary[\"total\"] / summary[\"count\"]\n",
        "    max_transaction = max(summary[\"transactions\"])\n",
        "    min_transaction = min(summary[\"transactions\"])\n",
        "\n",
        "    print(f\"  User {user_id}:\")\n",
        "    print(f\"    Total spent: ${summary['total']:.2f}\")\n",
        "    print(f\"    Transactions: {summary['count']}\")\n",
        "    print(f\"    Average transaction: ${avg_transaction:.2f}\")\n",
        "    print(f\"    Max transaction: ${max_transaction:.2f}\")\n",
        "    print(f\"    Min transaction: ${min_transaction:.2f}\")\n",
        "    print(f\"    Categories: {', '.join(summary['categories'])}\")\n",
        "\n",
        "# Category analysis across all users\n",
        "category_analysis = (transactions\n",
        "    .map(lambda t: (t[\"category\"], t[\"amount\"]))\n",
        "    .aggregateByKey(\n",
        "        {\"total\": 0, \"count\": 0, \"amounts\": []},\n",
        "        lambda acc, amount: {\n",
        "            \"total\": acc[\"total\"] + amount,\n",
        "            \"count\": acc[\"count\"] + 1,\n",
        "            \"amounts\": acc[\"amounts\"] + [amount]\n",
        "        },\n",
        "        lambda acc1, acc2: {\n",
        "            \"total\": acc1[\"total\"] + acc2[\"total\"],\n",
        "            \"count\": acc1[\"count\"] + acc2[\"count\"],\n",
        "            \"amounts\": acc1[\"amounts\"] + acc2[\"amounts\"]\n",
        "        }\n",
        "    )\n",
        "    .mapValues(lambda stats: {\n",
        "        **stats,\n",
        "        \"average\": stats[\"total\"] / stats[\"count\"],\n",
        "        \"max\": max(stats[\"amounts\"]),\n",
        "        \"min\": min(stats[\"amounts\"])\n",
        "    }))\n",
        "\n",
        "print(\"\\nCategory analysis:\")\n",
        "for category, stats in category_analysis.collect():\n",
        "    print(f\"  {category}:\")\n",
        "    print(f\"    Total: ${stats['total']:.2f}\")\n",
        "    print(f\"    Count: {stats['count']}\")\n",
        "    print(f\"    Average: ${stats['average']:.2f}\")\n",
        "    print(f\"    Range: ${stats['min']:.2f} - ${stats['max']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11oxn6IT634G",
        "outputId": "1a24cd54-9db3-4b75-c958-2d064d6fd4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PATTERN: ADVANCED AGGREGATION ===\n",
            "User spending summaries:\n",
            "  User 1:\n",
            "    Total spent: $480.00\n",
            "    Transactions: 4\n",
            "    Average transaction: $120.00\n",
            "    Max transaction: $300.00\n",
            "    Min transaction: $30.00\n",
            "    Categories: shopping, transport, food\n",
            "  User 2:\n",
            "    Total spent: $280.00\n",
            "    Transactions: 2\n",
            "    Average transaction: $140.00\n",
            "    Max transaction: $200.00\n",
            "    Min transaction: $80.00\n",
            "    Categories: transport, food\n",
            "  User 3:\n",
            "    Total spent: $175.00\n",
            "    Transactions: 2\n",
            "    Average transaction: $87.50\n",
            "    Max transaction: $150.00\n",
            "    Min transaction: $25.00\n",
            "    Categories: entertainment, food\n",
            "\n",
            "Category analysis:\n",
            "  food:\n",
            "    Total: $355.00\n",
            "    Count: 4\n",
            "    Average: $88.75\n",
            "    Range: $25.00 - $200.00\n",
            "  transport:\n",
            "    Total: $130.00\n",
            "    Count: 2\n",
            "    Average: $65.00\n",
            "    Range: $50.00 - $80.00\n",
            "  entertainment:\n",
            "    Total: $150.00\n",
            "    Count: 1\n",
            "    Average: $150.00\n",
            "    Range: $150.00 - $150.00\n",
            "  shopping:\n",
            "    Total: $300.00\n",
            "    Count: 1\n",
            "    Average: $300.00\n",
            "    Range: $300.00 - $300.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Time Series Analysis Pattern**"
      ],
      "metadata": {
        "id": "jXkJ9w9C7AYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PATTERN: TIME SERIES ANALYSIS ===\")\n",
        "\n",
        "# Sample time series data\n",
        "time_series_data = sc.parallelize([\n",
        "    {\"date\": \"2023-01-01\", \"value\": 100, \"metric\": \"sales\"},\n",
        "    {\"date\": \"2023-01-02\", \"value\": 120, \"metric\": \"sales\"},\n",
        "    {\"date\": \"2023-01-03\", \"value\": 90, \"metric\": \"sales\"},\n",
        "    {\"date\": \"2023-01-04\", \"value\": 150, \"metric\": \"sales\"},\n",
        "    {\"date\": \"2023-01-05\", \"value\": 200, \"metric\": \"sales\"},\n",
        "    {\"date\": \"2023-01-01\", \"value\": 50, \"metric\": \"visits\"},\n",
        "    {\"date\": \"2023-01-02\", \"value\": 60, \"metric\": \"visits\"},\n",
        "    {\"date\": \"2023-01-03\", \"value\": 45, \"metric\": \"visits\"},\n",
        "    {\"date\": \"2023-01-04\", \"value\": 75, \"metric\": \"visits\"},\n",
        "    {\"date\": \"2023-01-05\", \"value\": 100, \"metric\": \"visits\"}\n",
        "])\n",
        "\n",
        "# Daily aggregation\n",
        "daily_totals = (time_series_data\n",
        "    .map(lambda record: (record[\"date\"], record[\"value\"]))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortByKey())\n",
        "\n",
        "print(\"Daily totals:\")\n",
        "for date, total in daily_totals.collect():\n",
        "    print(f\"  {date}: {total}\")\n",
        "\n",
        "# Metric-wise analysis\n",
        "metric_analysis = (time_series_data\n",
        "    .map(lambda record: (record[\"metric\"], record[\"value\"]))\n",
        "    .groupByKey()\n",
        "    .mapValues(lambda values: {\n",
        "        \"values\": list(values),\n",
        "        \"total\": sum(values),\n",
        "        \"count\": len(list(values)),\n",
        "        \"average\": sum(values) / len(list(values)),\n",
        "        \"trend\": \"increasing\" if list(values)[-1] > list(values)[0] else \"decreasing\"\n",
        "    }))\n",
        "\n",
        "print(\"\\nMetric analysis:\")\n",
        "for metric, stats in metric_analysis.collect():\n",
        "    print(f\"  {metric}:\")\n",
        "    print(f\"    Total: {stats['total']}\")\n",
        "    print(f\"    Average: {stats['average']:.1f}\")\n",
        "    print(f\"    Trend: {stats['trend']}\")\n",
        "    print(f\"    Values: {stats['values']}\")\n",
        "\n",
        "# Moving averages (simplified 3-day window)\n",
        "def calculate_moving_average(partition):\n",
        "    data = sorted(list(partition), key=lambda x: x[\"date\"])\n",
        "    result = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        if i >= 2:  # 3-day window\n",
        "            window_sum = sum(data[j][\"value\"] for j in range(i-2, i+1))\n",
        "            moving_avg = window_sum / 3\n",
        "            result.append({\n",
        "                \"date\": data[i][\"date\"],\n",
        "                \"value\": data[i][\"value\"],\n",
        "                \"moving_avg\": moving_avg\n",
        "            })\n",
        "\n",
        "    return result\n",
        "\n",
        "sales_data = time_series_data.filter(lambda x: x[\"metric\"] == \"sales\")\n",
        "moving_averages = sales_data.mapPartitions(calculate_moving_average)\n",
        "\n",
        "print(\"\\nSales with 3-day moving average:\")\n",
        "for record in moving_averages.collect():\n",
        "    print(f\"  {record['date']}: Value={record['value']}, MA={record['moving_avg']:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YXQ-zht6-CX",
        "outputId": "f47227c4-bf5a-45ef-d92e-fa1463cc80a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PATTERN: TIME SERIES ANALYSIS ===\n",
            "Daily totals:\n",
            "  2023-01-01: 150\n",
            "  2023-01-02: 180\n",
            "  2023-01-03: 135\n",
            "  2023-01-04: 225\n",
            "  2023-01-05: 300\n",
            "\n",
            "Metric analysis:\n",
            "  visits:\n",
            "    Total: 330\n",
            "    Average: 66.0\n",
            "    Trend: increasing\n",
            "    Values: [50, 60, 45, 75, 100]\n",
            "  sales:\n",
            "    Total: 660\n",
            "    Average: 132.0\n",
            "    Trend: increasing\n",
            "    Values: [100, 120, 90, 150, 200]\n",
            "\n",
            "Sales with 3-day moving average:\n",
            "  2023-01-03: Value=90, MA=103.3\n",
            "  2023-01-04: Value=150, MA=120.0\n",
            "  2023-01-05: Value=200, MA=146.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📚 Key Takeaways:\n",
        "\n",
        "1. **Transformations are lazy** - They build up a computation plan\n",
        "2. **Actions are eager** - They trigger execution of the entire plan  \n",
        "3. **RDDs are immutable** - Operations create new RDDs\n",
        "4. **Use appropriate operations** - reduceByKey vs groupByKey, coalesce vs repartition\n",
        "5. **Cache frequently used RDDs** - Avoid recomputation\n",
        "6. **Be careful with collect()** - Can crash driver with large datasets\n",
        "\n",
        "🚀 Performance Best Practices:\n",
        "\n",
        "- Prefer `reduceByKey` over `groupByKey`\n",
        "- Use `mapPartitions` for expensive setup operations  \n",
        "- Cache RDDs that are used multiple times\n",
        "- Use appropriate number of partitions\n",
        "- Avoid unnecessary shuffles\n",
        "- Use built-in functions when available\n",
        "\n",
        "🎯 Common Operations by Use Case:\n",
        "\n",
        "| Use Case | Operations |\n",
        "|----------|------------|\n",
        "| Data Cleaning | filter, map, distinct |\n",
        "| Aggregation | reduceByKey, groupByKey, aggregate |\n",
        "| Transformation | map, flatMap, mapPartitions |\n",
        "| Analysis | count, countByValue, top, take |\n",
        "| Joining | join, leftOuterJoin, rightOuterJoin |\n"
      ],
      "metadata": {
        "id": "25HESXx_7MvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory usage monitoring\n",
        "import psutil\n",
        "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Display DataFrames nicely\n",
        "def show_rdd_sample(rdd, n=5):\n",
        "    \"\"\"Display first n elements of RDD in a nice format\"\"\"\n",
        "    sample = rdd.take(n)\n",
        "    for i, item in enumerate(sample, 1):\n",
        "        print(f\"{i:2d}: {item}\")\n",
        "\n",
        "# Save RDD to file (useful in Colab)\n",
        "def save_rdd_to_colab(rdd, filename):\n",
        "    \"\"\"Save RDD results to a file in Colab\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for item in rdd.collect():\n",
        "            f.write(str(item) + '\\n')\n",
        "    print(f\"Saved to {filename}\")\n",
        "\n",
        "# Load data from Colab files\n",
        "def load_text_file_to_rdd(filename):\n",
        "    \"\"\"Load a text file from Colab into RDD\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return sc.parallelize([line.strip() for line in lines])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_m5St27CVD",
        "outputId": "2a191cc5-e2a6-451f-f60b-f7be2df52d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage: 14.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DdkcHbGc_Hv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}